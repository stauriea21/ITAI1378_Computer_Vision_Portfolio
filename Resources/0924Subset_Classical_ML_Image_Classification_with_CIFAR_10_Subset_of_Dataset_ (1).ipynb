{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuFNS5pAGtbN"
      },
      "source": [
        "# Image Classification with CIFAR-10 Dataset and Scikit-Learn\n",
        "\n",
        "In this notebook, we will explore the basics of image classification using the CIFAR-10 dataset and build a simple image classifier using Scikit-Learn. We will perform the following steps:\n",
        "1. Load and preprocess the CIFAR-10 dataset.\n",
        "2. Extract features from the images.\n",
        "3. Train a machine learning model on the extracted features.\n",
        "4. Evaluate the model's performance.\n",
        "\n",
        "## Step 1: Installing and Importing Libraries\n",
        "\n",
        "### Installing Libraries\n",
        "\n",
        "Before we start, we need to ensure that we have all the necessary libraries installed. We can use `!pip install` to install any missing libraries directly from the Jupyter Notebook. This command is useful for installing Python packages from the Python Package Index (PyPI).\n",
        "\n",
        "### Why Install Libraries?\n",
        "\n",
        "Libraries provide pre-written code that we can use to perform various tasks without having to write everything from scratch. Installing libraries ensures we have access to the necessary functions and tools needed for our project.\n",
        "\n",
        "Here are the libraries we need for this notebook:\n",
        "- **numpy:** For numerical operations.\n",
        "- **matplotlib:** For plotting and visualizing images.\n",
        "- **tensorflow:** For loading the CIFAR-10 dataset.\n",
        "- **scikit-learn (sklearn):** For machine learning models and evaluation metrics.\n",
        "\n",
        "### Installing Libraries Individually vs. All at Once\n",
        "\n",
        "\n",
        "You can install each library separately by writing a `!pip install` command for each library. This looks like:\n",
        "```python\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install tensorflow\n",
        "!pip install scikit-learn\n",
        "\n",
        "\n",
        "Alternatively, you can install all the libraries together in a single !pip install command by separating the library names with a space\n",
        "\n",
        "!pip install numpy matplotlib tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53kEJohdGtbP"
      },
      "source": [
        "## Differences and Considerations\n",
        "- **Convenience**: Installing all libraries at once is more convenient and requires fewer lines of code. It can save time when writing and running the notebook.\n",
        "- **Execution Time**: Running a single !pip install command can be faster than running multiple commands, as it reduces the overhead of initiating separate installation processes for each library.\n",
        "- **Dependency Management**: Installing libraries together can help Pip resolve dependencies more efficiently, potentially avoiding conflicts that might arise when installing libraries separately.\n",
        "- **Debugging**: Installing libraries individually can make it easier to identify which specific library caused an issue if an installation error occurs. However, this is generally only a concern if you encounter frequent installation problems.\n",
        "\n",
        "For this notebook, we will install all necessary libraries together for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNVFNL-TGtbP"
      },
      "outputs": [],
      "source": [
        "!pip install numpy matplotlib tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nem379f3GtbQ"
      },
      "source": [
        "\n",
        "### Importing Libraries\n",
        "\n",
        "After installing the libraries, we need to import them into our notebook. Importing libraries allows us to use their functionalities in our code. Each library is imported using an alias (short name) to make the code cleaner and more readable.\n",
        "\n",
        "Let's import the required libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrsWE2KUGtbQ"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58W-d1p4GtbQ"
      },
      "source": [
        "## Step 2: Loading and Preprocessing the CIFAR-10 Dataset\n",
        "\n",
        "The CIFAR-10 dataset is readily available in the `keras` library. We will load the dataset and preprocess it by converting the images to grayscale and flattening them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGyvINBZGtbQ"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# To minimize computational demands lets work with three classes of your choice\n",
        "\n",
        "# CIFAR-10 classes\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "# Choose a subset of classes\n",
        "chosen_classes = ['cat', 'dog', 'ship']\n",
        "class_indices = [class_names.index(cls) for cls in chosen_classes]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter data for the chosen classes\n",
        "mask_train = np.isin(y_train, class_indices)\n",
        "mask_test = np.isin(y_test, class_indices)\n",
        "X_train_subset = X_train[mask_train.flatten()]\n",
        "y_train_subset = y_train[mask_train]\n",
        "X_test_subset = X_test[mask_test.flatten()]\n",
        "y_test_subset = y_test[mask_test]\n"
      ],
      "metadata": {
        "id": "iQ9BrCg4JAA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert images to grayscale\n",
        "X_train_gray = np.dot(X_train_subset[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "X_test_gray = np.dot(X_test_subset[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "\n",
        "# Normalize the images\n",
        "X_train_normalized = X_train_gray / 255.0\n",
        "X_test_normalized = X_test_gray / 255.0\n",
        "\n",
        "# Flatten the images\n",
        "X_train_flat = X_train_normalized.reshape(X_train_normalized.shape[0], -1)\n",
        "X_test_flat = X_test_normalized.reshape(X_test_normalized.shape[0], -1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "48w3pey5Nt-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample image\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(X_train_gray[0], cmap='gray')\n",
        "plt.title(f'Sample Image: {chosen_classes[np.where(class_indices == y_train_subset[0])[0][0]]}')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(\"Training set size:\", X_train_flat.shape)\n",
        "print(\"Testing set size:\", X_test_flat.shape)"
      ],
      "metadata": {
        "id": "3LclpMIEPiow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n68rEJQ-GtbQ"
      },
      "source": [
        "## Step 3: Training a Machine Learning Model\n",
        "\n",
        "### What is SVM (Support Vector Machine)?\n",
        "\n",
        "Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification and regression tasks. However, it is mostly used for classification problems. The objective of the SVM algorithm is to find a hyperplane in an N-dimensional space (N is the number of features) that distinctly classifies the data points.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "- **Hyperplane:** A decision boundary that separates different classes in the feature space. In 2D, it's a line; in 3D, it's a plane.\n",
        "- **Support Vectors:** Data points that are closest to the hyperplane and influence its position and orientation. These points help in maximizing the margin of the classifier.\n",
        "- **Margin:** The distance between the hyperplane and the closest data points from either class. SVM aims to maximize this margin.\n",
        "\n",
        "### Why Use SVM?\n",
        "\n",
        "- **Effective in high-dimensional spaces:** SVM is very effective when the number of features is large.\n",
        "- **Memory efficient:** It uses a subset of training points (support vectors) in the decision function, making it memory efficient.\n",
        "- **Versatile:** Different kernel functions can be specified for the decision function. Common kernels include linear, polynomial, and radial basis function (RBF).\n",
        "\n",
        "### What does `SVC(kernel='linear')` mean?\n",
        "\n",
        "`SVC` stands for Support Vector Classification, which is a class in the Scikit-Learn library used to implement the SVM algorithm for classification tasks. The `kernel` parameter in the `SVC` class specifies the type of hyperplane used to separate the data.\n",
        "\n",
        "#### Kernel Types:\n",
        "\n",
        "- **Linear Kernel:** The data is linearly separable (i.e., a straight line or hyperplane can separate the data). This is the simplest kernel.\n",
        "  - When we use `SVC(kernel='linear')`, it means we are using a linear kernel for our SVM. This kernel is appropriate when the data can be separated by a straight line (or hyperplane in higher dimensions).\n",
        "- **Polynomial Kernel:** The data is not linearly separable, but a polynomial function of the input features can separate the data.\n",
        "- **Radial Basis Function (RBF) Kernel:** The data is not linearly separable, but mapping the data into a higher-dimensional space using a Gaussian (RBF) function can separate the data.\n",
        "\n",
        "### Training the SVM Model\n",
        "\n",
        "We will use a Support Vector Machine (SVM) classifier from Scikit-Learn to train our model on the extracted features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLz6LwlXGtbR"
      },
      "outputs": [],
      "source": [
        "# Train an SVM classifier\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train_flat, y_train_subset.ravel())\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test_flat)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test_subset, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_subset, y_pred, target_names=chosen_classes))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see some images of the  datasset in the different stages\n",
        "# Function to display images\n",
        "def display_images(images, titles, main_title, cmap=None):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "    fig.suptitle(main_title)\n",
        "    for i, ax in enumerate(axes):\n",
        "        if cmap:\n",
        "            ax.imshow(images[i], cmap=cmap)\n",
        "        else:\n",
        "            ax.imshow(images[i])\n",
        "        ax.set_title(titles[i])\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display original color images\n",
        "display_images(X_train_subset[:5],\n",
        "               [chosen_classes[np.where(class_indices == y)[0][0]] for y in y_train_subset[:5]],\n",
        "               'Original Color Images')\n",
        "\n",
        "# Display grayscale images\n",
        "display_images(X_train_gray[:5],\n",
        "               [chosen_classes[np.where(class_indices == y)[0][0]] for y in y_train_subset[:5]],\n",
        "               'Grayscale Images', cmap='gray')\n",
        "\n",
        "# Display normalized images\n",
        "display_images(X_train_normalized[:5],\n",
        "               [chosen_classes[np.where(class_indices == y)[0][0]] for y in y_train_subset[:5]],\n",
        "               'Normalized Grayscale Images', cmap='gray')"
      ],
      "metadata": {
        "id": "mRN0KnsgS7Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCflqDzOGtbR"
      },
      "source": [
        "## Step 4: Conclusion\n",
        "\n",
        "In this notebook, we:\n",
        "1. Loaded and preprocessed the CIFAR-10 dataset.\n",
        "2. Converted the images to grayscale and flattened them to use as features.\n",
        "3. Trained an SVM classifier on the extracted features.\n",
        "4. Evaluated the model's performance.\n",
        "\n",
        "### Summary of SVM\n",
        "\n",
        "Support Vector Machines (SVM) are a powerful tool for classification tasks. They work by finding the optimal hyperplane that maximizes the margin between different classes. The key points include:\n",
        "- **Hyperplane:** The decision boundary.\n",
        "- **Support Vectors:** Critical data points that define the hyperplane.\n",
        "- **Margin:** The gap between the hyperplane and the nearest data points from any class.\n",
        "\n",
        "SVMs are effective in high-dimensional spaces and are versatile due to the use of different kernel functions. However, they can be computationally intensive for large datasets and less effective for overlapping classes.\n",
        "\n",
        "This exercise provided a basic introduction to image classification using classical machine learning techniques. Next  few modules will explore advanced applications, using deep learning models like Convolutional Neural Networks (CNNs) using libraries such as TensorFlow or PyTorch.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}